# -*- coding: utf-8 -*-
"""IRIN THOMAS Python Programming File.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sQlF1IoJRJjcLw4LT61DI1AJyflz_M56

## Question 1

# 1st Part
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Load data
df = pd.read_csv( '/content/Marketing Campaign data (1).csv')
print(df.head())

print(df.dtypes)

def get_description(column):
    descriptions = {
        'CUST_ID': 'Unique customer identifier',
        'CUST_GENDER': 'Customer gender (M/F)',
        'AGE': 'Customer age in years',
        'CUST_MARITAL_STATUS': 'Customer marital status',
        'COUNTRY_NAME': 'Country of residence',
        'CUST_INCOME_LEVEL': 'Income level category',
        'EDUCATION': 'Education level',
        'OCCUPATION': 'Occupation category',
        'HOUSEHOLD_SIZE': 'Number of people in household',
        'YRS_RESIDENCE': 'Years at current residence',
        'AFFINITY_CARD': 'Target variable (1=High-value, 0=Low-value)',
        'BULK_PACK_DISKETTES': 'Purchased bulk pack diskettes',
        'FLAT_PANEL_MONITOR': 'Purchased flat panel monitor',
        'HOME_THEATER_PACKAGE': 'Purchased home theater package',
        'BOOKKEEPING_APPLICATION': 'Purchased bookkeeping application',
        'PRINTER_SUPPLIES': 'Purchased printer supplies',
        'Y_BOX_GAMES': 'Purchased Y-box games',
        'OS_DOC_SET_KANJI': 'Purchased OS doc set in Kanji',
        'COMMENTS': 'Free text customer comments'
    }
    return descriptions.get(column, 'No description available')

import numpy as np
import matplotlib.pyplot as plt

def generate_numeric_metadata(df):
    # Define numeric_columns first
    numeric_columns = df.select_dtypes(include=['int64']).columns
    metadata = []

    for column in numeric_columns:
        col_data = df[column]
        stats = {
            'Attribute': column,
            'Description': get_description(column),
            'Max': col_data.max(),
            'Min': col_data.min(),
            'Mean': col_data.mean(),
            'Std Dev': col_data.std(),

        }

        plt.figure(figsize=(8, 4))
        col_data.hist(bins=20, edgecolor='black')
        plt.title(f'Distribution of {column}')
        plt.xlabel(column)
        plt.ylabel('Frequency')
        #plt.savefig(f'hist_{column}.png', dpi=300, bbox_inches='tight')
        plt.close()

        metadata.append(stats)
    return pd.DataFrame(metadata)

def generate_categorical_metadata(df):
    # Define categorical_columns first
    categorical_columns = df.select_dtypes(include=['object', 'category']).columns
    metadata = []

    for column in categorical_columns:
        col_data = df[column]
        mode_value = col_data.mode()[0] if not col_data.mode().empty else 'N/A'

        stats = {
            'Attribute': column,
            'Description': get_description(column),
            'Mode': mode_value,

        }

        plt.figure(figsize=(8, 4))
        col_data.value_counts().plot(kind='bar', edgecolor='black')
        plt.title(f'Value Counts of {column}')
        plt.xlabel(column)
        plt.ylabel('Count')
        plt.xticks(rotation=45)
        plt.plot()
        plt.close()

        metadata.append(stats)

    return pd.DataFrame(metadata)

numeric_meta = generate_numeric_metadata(df)
categorical_meta = generate_categorical_metadata(df)

print("Numeric Attributes Metadata:")
print(numeric_meta.to_markdown(index=False))

print("\nCategorical Attributes Metadata:")
print(categorical_meta.to_markdown(index=False))

import seaborn as sns
# Set the style for better looking plots
plt.style.use('ggplot')

# 1. Plot histograms for all numeric columns
numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns

print("Displaying histograms for numeric columns:")
for col in numeric_cols:
    plt.figure(figsize=(5, 2))
    df[col].hist(bins=20, color='skyblue', edgecolor='black')
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.grid(False)
    plt.tight_layout()
    plt.show()
    print(f"Displayed histogram for {col}")

# 2. Plot bar charts for all categorical columns
categorical_cols = df.select_dtypes(include=['object', 'category']).columns

print("\nDisplaying bar charts for categorical columns:")
for col in categorical_cols:
    # Skip columns with too many unique values (like comments)
    if df[col].nunique() > 20:
        print(f"- Skipping {col} (too many unique values)")
        continue

    plt.figure(figsize=(5, 2))
    value_counts = df[col].value_counts()
    ax = value_counts.plot(kind='bar', color='lightgreen', edgecolor='black')

    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Count')
    plt.xticks(rotation=45, ha='right')

    # Add count labels on top of bars
    for i, v in enumerate(value_counts):
        ax.text(i, v + 0.5, str(v), ha='center', va='bottom')

    plt.tight_layout()
    plt.show()
    print(f"Displayed bar chart for {col}")

print("\nAll visualizations displayed successfully!")

"""# Part 2"""

print(df['AGE'].unique())
print(df['CUST_GENDER'].unique())
print(df['CUST_MARITAL_STATUS'].unique())
print(df['COUNTRY_NAME'].unique())
print(df['CUST_INCOME_LEVEL'].unique())
print(df['EDUCATION'].unique())
print(df['OCCUPATION'].unique())
print(df['HOUSEHOLD_SIZE'].unique())
print(df['YRS_RESIDENCE'].unique())
print(df['AFFINITY_CARD'].unique())
print(df['BULK_PACK_DISKETTES'].unique())
print(df['FLAT_PANEL_MONITOR'].unique())
print(df['HOME_THEATER_PACKAGE'].unique())
print(df['BOOKKEEPING_APPLICATION'].unique())
print(df['PRINTER_SUPPLIES'].unique())
print(df['Y_BOX_GAMES'].unique())
print(df['OS_DOC_SET_KANJI'].unique())

import pandas as pd
import numpy as np

def analyze_data_issues(df):
    # Initialize results storage
    missing_results = []
    error_results = []

    # Define expected valid values for each column
    valid_values = {
        'CUST_GENDER': ['M', 'F'],
        'CUST_MARITAL_STATUS': ['Married', 'Single', 'Divorced', 'Widowed', 'NeverM', 'Separated'],
        'EDUCATION': ['High School', 'Bachelors', 'Masters', 'PhD', 'Associates', 'Vocational'],
        'HOUSEHOLD_SIZE': ['1', '2', '3', '4', '5', '6', '7', '8', '9'],
        'AFFINITY_CARD': [0, 1],
        'BULK_PACK_DISKETTES': [0, 1],
        'FLAT_PANEL_MONITOR': [0, 1],
        'HOME_THEATER_PACKAGE': [0, 1],
        'BOOKKEEPING_APPLICATION': [0, 1],
        'PRINTER_SUPPLIES': [0, 1],
        'Y_BOX_GAMES': [0, 1],
        'OS_DOC_SET_KANJI': [0, 1]
    }

    for column in df.columns:
        # Missing data detection
        null_count = df[column].isnull().sum()
        empty_count = (df[column] == '').sum() if df[column].dtype == 'object' else 0
        special_missing = 0

        if column == 'OCCUPATION':
            special_missing = (df[column] == '?').sum()

        total_missing = null_count + empty_count + special_missing

        if total_missing > 0:
            missing_results.append({
                'Attribute': column,
                'Null Values': null_count,
                'Empty Strings': empty_count,
                'Special Missing (?, etc)': special_missing,
                'Total Missing': total_missing,
                '% Missing': f"{(total_missing/len(df))*100:.2f}%",
                'Handling Suggestion': get_missing_handling_suggestion(column, total_missing, len(df))
            })

        # Error data detection
        errors = []

        # Check for invalid values in categorical columns
        if column in valid_values:
            invalid_values = ~df[column].isin(valid_values[column])
            invalid_count = invalid_values.sum()

            if invalid_count > 0:
                # Get top 3 most frequent invalid values
                top_invalid = df[column][invalid_values].value_counts().head(3)
                error_desc = f"{invalid_count} invalid values: " + ", ".join(
                    [f"'{val}' ({count})" for val, count in top_invalid.items()])
                errors.append(error_desc)

        # Specific column checks
        if column == 'CUST_MARITAL_STATUS':
            inconsistent_values = ['Divorc.', 'Mabsent', 'Separ.']
            invalid_count = df[column].isin(inconsistent_values).sum()
            if invalid_count > 0:
                errors.append(f"{invalid_count} inconsistent values (Divorc./Mabsent/Separ.)")

        if column == 'EDUCATION':
            abbreviated_values = ['Bach.', 'HS-grad', '< Bach.', 'Profsc', 'Assoc-A', 'Assoc-V']
            invalid_count = df[column].isin(abbreviated_values).sum()
            if invalid_count > 0:
                errors.append(f"{invalid_count} abbreviated/truncated education levels")

        if column == 'HOUSEHOLD_SIZE':
            invalid_formats = ['9+', '8-Jun']
            invalid_count = df[column].isin(invalid_formats).sum()
            if invalid_count > 0:
                errors.append(f"{invalid_count} inconsistent formats (9+/8-Jun)")

        if column == 'AGE':
            invalid_ages = (df[column] < 0) | (df[column] > 120)
            invalid_count = invalid_ages.sum()
            if invalid_count > 0:
                errors.append(f"{invalid_count} ages outside 0-120 range")

        if column == 'YRS_RESIDENCE':
            invalid_yrs = df[column] < 0
            invalid_count = invalid_yrs.sum()
            if invalid_count > 0:
                errors.append(f"{invalid_count} negative years of residence")

        if errors:
            error_results.append({
                'Attribute': column,
                'Error Types': "; ".join(errors),
                'Handling Suggestion': get_error_handling_suggestion(column, errors)
            })

    # Create DataFrames from results
    missing_df = pd.DataFrame(missing_results) if missing_results else pd.DataFrame()
    error_df = pd.DataFrame(error_results) if error_results else pd.DataFrame()

    return missing_df, error_df

def get_missing_handling_suggestion(column, missing_count, total_records):
    missing_pct = (missing_count / total_records) * 100

    if column == 'OCCUPATION':
        return "Create 'Unknown' category for '?' values or impute based on other customer attributes"

    if missing_pct > 30:
        return "Consider removing column (high missingness)"
    elif missing_pct > 5:
        if pd.api.types.is_numeric_dtype(df[column]):
            return "Median imputation or advanced imputation methods"
        else:
            return "Mode imputation or create 'Missing' category"
    else:
        if pd.api.types.is_numeric_dtype(df[column]):
            return "Mean/median imputation or remove records"
        else:
            return "Make 'Other Commnt' to store the records"

def get_error_handling_suggestion(column, errors):
    if column == 'CUST_MARITAL_STATUS':
        return "Standardize values: 'Divorc.'→'Divorced', 'Separ.'→'Separated', 'Mabsent'→'Separated'"

    if column == 'EDUCATION':
        return "Map to standard categories: 'Bach.'→'Bachelors', 'HS-grad'→'High School', etc."

    if column == 'HOUSEHOLD_SIZE':
        return "Convert '9+' to '9' and '8-Jun' to '8' (or investigate correct mapping)"

    if column in ['AGE', 'YRS_RESIDENCE']:
        return "Cap values at reasonable limits (0-120 for age, 0+ for residence years)"

    if column in valid_values:
        return "Remove records with invalid values or map to nearest valid category"

    return "Manual review required"

# Run the analysis
missing_df, error_df = analyze_data_issues(df)

# Display results
print("=== Missing Data Analysis ===")
print(missing_df.to_markdown(index=False) if not missing_df.empty else print("No missing data found"))

print("\n=== Error Data Analysis ===")
print(error_df.to_markdown(index=False) if not error_df.empty else print("No error data found"))

"""# Question 2"""

# Number of unique values in the column
n_unique = df['PRINTER_SUPPLIES'].nunique()
print("Unique values:", n_unique)

if n_unique > 1:
    print("Column has variance")
else:
    print("Column is constant (zero variance)")

df.drop(columns=['CUST_ID', 'PRINTER_SUPPLIES'], inplace=True)

print(df.columns)

"""# Part 2"""

# Mapping invalid marital status
marital_mapping = {
    'NeverM':'Never Married',
    'Married': 'Married',
    'Divorc.': 'Divorced',
    'Mabsent': 'Married-Apart',
    'Separ.': 'Separated',
    'Mar-AF': 'Married-Forces',
    'Widowed': 'Widowed'
}

df['CUST_MARITAL_STATUS'] = df['CUST_MARITAL_STATUS'].replace(marital_mapping)
df['CUST_MARITAL_STATUS'] = df['CUST_MARITAL_STATUS'].fillna('Other')

print(df['CUST_MARITAL_STATUS'].unique())

print(df['BOOKKEEPING_APPLICATION'].unique())
print(df['YRS_RESIDENCE'].unique())
print(df['OS_DOC_SET_KANJI'].unique())

# Confirm null values in COMMENTS
print(df['COMMENTS'].isnull().sum())  # Should print 73

# Handle invalid entries in 'HOUSEHOLD_SIZE'
df['HOUSEHOLD_SIZE'] = df['HOUSEHOLD_SIZE'].replace({
    '9+': '9',
    '8-Jun': '8',
    '5-Apr': '5'
}).astype(int)

# Confirm all fixes
print("Unique values in HOUSEHOLD_SIZE after cleanup:", df['HOUSEHOLD_SIZE'].unique())

print("Missing values after cleanup:\n", df.isnull().sum())

# 3. Normalize OCCUPATION casing
df['OCCUPATION'] = df['OCCUPATION'].str.title()
# Justification: ensures occupations like “sales” and “Sales” merge into one category.

df['HOUSEHOLD_SIZE'] = df['HOUSEHOLD_SIZE'].astype(int)


numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
for col in numeric_cols:
    # Coerce errors and count
    errors = pd.to_numeric(df[col], errors='coerce').isna().sum()
    assert errors == 0, f"Found {errors} invalid entries in {col}"
# Justification: catches any remaining stray non-numeric entries early.

# 6. Check final missingness
print("Missing values after cleaning:")
print(df.isnull().sum())

# COMMENTS remains untouched for text analysis in a later part.

print(df.head())

"""# Part 3"""

df['CUST_GENDER'] = df['CUST_GENDER'].map({'F': 0, 'M': 1})
print(df['CUST_GENDER'])

print(df.head())

country_freq = df['COUNTRY_NAME'].value_counts()
country_rank = {country: rank+1 for rank, country in enumerate(country_freq.index)}
df['COUNTRY_NAME'] = df['COUNTRY_NAME'].map(country_rank)
print(country_freq)
print(df['COUNTRY_NAME'])

print(df['CUST_INCOME_LEVEL'].dropna().unique())

income_level_map = {
    'A: Below 30,000': 1,
    'B: 30,000 - 49,999': 1,
    'C: 50,000 - 69,999': 2,
    'D: 70,000 - 89,999': 2,
    'E: 90,000 - 109,999': 2,
    'F: 110,000 - 129,999': 3,
    'G: 130,000 - 149,999': 3,
    'H: 150,000 - 169,999': 3,
    'I: 170,000 - 189,999': 3,
    'J: 190,000 - 249,999': 3,
    'K: 250,000 - 299,999': 3,
    'L: 300,000 and above': 3
}

# Apply mapping
df['CUST_INCOME_LEVEL'] = df['CUST_INCOME_LEVEL'].map(income_level_map)

# Show result
print("Mapped income levels:")
print(df['CUST_INCOME_LEVEL'].value_counts(dropna=False))

print(df['CUST_INCOME_LEVEL'].unique() )

print(df['EDUCATION'].unique())

df['EDUCATION'] = df['EDUCATION'].replace({
    '< Bach.': 'Less than Bachelors',
    'Presch.': 'Preschool',
    'HS-grad': 'High School',
    'Assoc-V': 'Associate V',
    'Assoc-A': 'Associate of Arts',
    'Profsc': 'Professional',
    'Bach.': 'Bachelors',


})

print(df['EDUCATION'].unique())

education_map = {
    'Preschool': 1,
    '1st-4th': 2,
    '5th-6th': 3,
    '7th-8th': 4,
    '9th': 5,
    '10th': 6,
    '11th': 7,
    '12th': 8,
    'Less than Bachelors': 9,
    'High School': 10,
    'Associate of Arts': 11,
    'Associate V': 12,
    'Bachelors': 13,
    'Masters': 14,
    'PhD': 15,
    'Professional': 16
}
df['EDUCATION'] = df['EDUCATION'].map(education_map)
print(df['EDUCATION'])

print(df['EDUCATION'].unique())

print(df['OCCUPATION'].unique())

df['OCCUPATION'] = df['OCCUPATION'].replace({
    'Prof.': 'Professional',
    'Sales': 'Sales',
    'Cleric.': 'Clerical',
    'Exec.': 'Executive',
    'Other': 'Other',
    'Farming': 'Farming',
    'Transp.': 'Transportation',
    'Machine': 'Machine Operator',
    'Crafts': 'Craftsman',
    'Handler': 'Handler / Laborer',
    '?': 'Unknown',
    'Protec.': 'Protective Services',
    'TechSup': 'Technical Support',
    'House-s': 'Homemaker / House-spouse',
    'Armed-F': 'Armed Forces'
})

print(df['OCCUPATION'].unique())

# One-hot encode 'OCCUPATION' and keep the original column
one_hot_encoded = pd.get_dummies(df['OCCUPATION'], prefix='OCCUPATION', dtype=int)

# Concatenate the original DataFrame with the one-hot encoded DataFrame
df = pd.concat([df, one_hot_encoded], axis=1)

print(df['OCCUPATION'])

occupation_columns = [col for col in df.columns if col.startswith('OCCUPATION')]
print(occupation_columns)

print(df.head())

"""# Question 3 Part 1"""

print(df.dtypes)

# Convert specified columns from float to int
columns_to_convert = ['YRS_RESIDENCE', 'AGE','BOOKKEEPING_APPLICATION', 'OS_DOC_SET_KANJI', ]  # Add other columns as needed

for column in columns_to_convert:
    df[column] = df[column].astype(int)
print(df.dtypes)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['CUST_MARITAL_STATUS'] = le.fit_transform(df['CUST_MARITAL_STATUS'])
print(df['CUST_MARITAL_STATUS'])
df['OCCUPATION'] = le.fit_transform(df['OCCUPATION'])
print(df['OCCUPATION'])

print(df.dtypes)

df_copy1 = df.copy()

df_copy1 = df_copy1.drop(columns=['OS_DOC_SET_KANJI', 'BOOKKEEPING_APPLICATION', 'OCCUPATION_Armed Forces', 'OCCUPATION_Clerical', 'OCCUPATION_Craftsman',
                                  'OCCUPATION_Executive', 'OCCUPATION_Farming', 'OCCUPATION_Handler / Laborer', 'OCCUPATION_House-s', 'OCCUPATION_Machine Operator', 'OCCUPATION_Other',
                                  'OCCUPATION_Professional', 'OCCUPATION_Protective Services', 'OCCUPATION_Sales', 'OCCUPATION_Techsup',
                                  'OCCUPATION_Transportation', 'OCCUPATION_Unknown','OCCUPATION_House-S'], errors='ignore')

print(df_copy1.columns)

df_numeric = df_copy1.drop(columns=['COMMENTS'])
print(df_numeric.columns)

# Calculate the correlation matrix of the remaining columns
correlation_matrix = df_numeric.corr()

print(correlation_matrix)

plt.figure(figsize=(15, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()

"""# Part 2 Question 3"""

print(df_copy1.columns)

print(df.columns)

#!pip install textblob
from textblob import TextBlob

# Function to classify sentiment based on polarity
def get_sentiment(text):
    if not isinstance(text, str) or text.strip() == '':
        return 'Neutral' # Treat empty or non-string comments as neutral
    polarity = TextBlob(text).sentiment.polarity
    if polarity > 0.1:
        return 'Positive'  # Positive
    elif polarity < -0.1:
        return 'Negative'  # Negative
    else:
        return 'Neutral' # Neutral

# Apply the sentiment function to the COMMENTS column
df_copy1['COMMENTS_SENTIMENT'] =df_copy1['COMMENTS'].apply(get_sentiment)

# Show a few examples
print(df_copy1[['COMMENTS', 'COMMENTS_SENTIMENT']].head(10))

print(df_copy1['COMMENTS_SENTIMENT'].unique())

"""# Question 4"""

def interactive_histogram_viewer(df_copy1):

    # Get list of numeric columns
    numeric_cols = df_copy1.select_dtypes(include=['int64', 'float64']).columns.tolist()

    if not numeric_cols:
        print("No numeric variables found in the dataset!")
        return

    while True:
        print("\nAvailable numeric variables:")
        for i, col in enumerate(numeric_cols, 1):
            print(f"{i}. {col}")
        print("0. Exit")

        try:
            choice = int(input("\nEnter the number of variable to plot (or 0 to exit): "))

            if choice == 0:
                print("Exiting histogram viewer...")
                break

            if 1 <= choice <= len(numeric_cols):
                selected_col = numeric_cols[choice-1]

                # Create histogram
                plt.figure(figsize=(10, 6))
                df[selected_col].hist(bins=20, edgecolor='black')
                plt.title(f'Distribution of {selected_col}')
                plt.xlabel(selected_col)
                plt.ylabel('Frequency')
                plt.grid(False)
                plt.show()

                print(f"\nHistogram for '{selected_col}' displayed.")
            else:
                print("Invalid choice. Please enter a number from the list.")

        except ValueError:
            print("Please enter a valid number.")

interactive_histogram_viewer(df_copy1)

print(df_copy1.columns)

"""To check if the comment sentiment should be kept for model fitting"""

df_copy2 = df_copy1.copy()

# One-hot encoding
one_hot_encoded = pd.get_dummies(df_copy2['COMMENTS_SENTIMENT'], prefix='sentiment',dtype=int)

# Combine the one-hot encoded columns with the original DataFrame (excluding the original column)
df_copy2 = pd.concat([df_copy2.drop('COMMENTS_SENTIMENT', axis=1), one_hot_encoded], axis=1)

print(df_copy2)

columns_numeric= df_copy2.drop(columns=['COMMENTS'])

correlation_matrix = columns_numeric.corr()
plt.figure(figsize=(15, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()

"""there is hardly any corelation so we are not keeping the comment and the sentimental analysis part in the model trainaing

"""

df_copy1= df_copy1.drop(columns=['COMMENTS', 'COMMENTS_SENTIMENT'], errors='ignore')

"""# Question 5"""

print(df_copy1.columns)

"""the dataframe to be uesd here is df_copy1

# Part1
"""

from sklearn.feature_selection import mutual_info_classif

# Drop non-predictive fields
X_train_all = df_copy1.drop(columns=['AFFINITY_CARD'], errors='ignore')
y_train_all = df_copy1['AFFINITY_CARD']

# Compute mutual information
mi_scores = mutual_info_classif(X_train_all, y_train_all, discrete_features='auto', random_state=42)

# Create dataframe of feature scores
mi_df = pd.DataFrame({
    'Feature': X_train_all.columns,
    'MI Score': mi_scores
}).sort_values(by='MI Score', ascending=False)

# Get top 10 features
top_10_features = mi_df['Feature'].head(10).tolist()

print("Top 10 features used in prediction UI:")
print(top_10_features)

from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import mutual_info_classif
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np

# 1. Separate test and training data
df_test = df_copy1.iloc[:100].copy()
df_train = df_copy1.iloc[100:].copy()

# 2. Prepare data: Drop COMMENTS and keep numerical/binary columns
X_train = df_train.drop(columns=['AFFINITY_CARD'], errors='ignore')
y_train = df_train['AFFINITY_CARD']

X_top_train = X_train[top_10_features]

# Standardize features for better model performance (optional but recommended)
scaler = StandardScaler()
X_top_train_scaled = scaler.fit_transform(X_top_train)

log_reg = LogisticRegression(max_iter=1000, random_state=42)
log_reg.fit(X_top_train_scaled, y_train)

# 6. Display intercept and coefficients
print("\nLogistic Regression Intercept:")
print(log_reg.intercept_[0])

print("\nLogistic Regression Coefficients:")
for feature, coef in zip(top_10_features, log_reg.coef_[0]):
    print(f"{feature}: {coef:.4f}")

"""# Part 2"""

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Prepare test data
X_test = df_test[top_10_features]
y_test = df_test['AFFINITY_CARD']

# Apply the same scaler used in training
X_test_scaled = scaler.transform(X_test)

# 2. Make predictions
y_pred = log_reg.predict(X_test_scaled)
y_pred_proba = log_reg.predict_proba(X_test_scaled)[:, 1]  # Probability estimates for ROC

# 3. Evaluate accuracy and classification metrics
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy on test set (first 100 customers): {accuracy:.2%}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# 5. Plot ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})', color='darkorange')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.grid(True)
plt.tight_layout()
plt.show()

# 4. Plot Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Pred 0', 'Pred 1'], yticklabels=['Actual 0', 'Actual 1'])
plt.title("Confusion Matrix")
plt.ylabel("Actual")
plt.xlabel("Predicted")
plt.tight_layout()
plt.show()

print(top_10_features)

"""# New section"""

import pandas as pd
import numpy as np
from sklearn.feature_selection import mutual_info_classif
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

# Assume df_copy1 is your fully preprocessed DataFrame, with 'AFFINITY_CARD' as target
# e.g. df_copy1 = pd.read_csv('processed_campaign_data.csv')

# 1. Split into test (first 100) and train (rest)
df_test  = df_copy1.iloc[:100].reset_index(drop=True)
df_train = df_copy1.iloc[100:].reset_index(drop=True)

# 2. Prepare X/y for training
X_train_all = df_train.drop(columns=['AFFINITY_CARD'])
y_train     = df_train['AFFINITY_CARD']

# 3. Compute Mutual Information to rank features
mi_scores = mutual_info_classif(X_train_all, y_train, discrete_features='auto', random_state=42)
mi_df = pd.DataFrame({
    'feature': X_train_all.columns,
    'mi_score': mi_scores
}).sort_values('mi_score', ascending=False)

# Select top 10
top_10 = mi_df['feature'].head(10).tolist()
print("Top 10 features:", top_10)

# 4. Scale those features
scaler = StandardScaler()
X_top_train = X_train_all[top_10]
X_train_scaled = scaler.fit_transform(X_top_train)

# 5. Fit logistic regression
clf = LogisticRegression(max_iter=1000, random_state=42)
clf.fit(X_train_scaled, y_train)

# 6. Output intercept & coefficients
print("\nIntercept:")
print(clf.intercept_[0])

print("\nFeature Coefficients:")
for feat, coef in zip(top_10, clf.coef_[0]):
    print(f"  {feat:20s}  {coef:+.4f}")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, roc_curve, auc, confusion_matrix

# 1. Define test set
df_test = df_copy1.iloc[:100].reset_index(drop=True)
X_test  = df_test[top_10]
y_test  = df_test['AFFINITY_CARD']

# 2. Scale using your existing scaler
X_test_scaled = scaler.transform(X_test)

# 3. Predict probabilities and labels
y_proba = clf.predict_proba(X_test_scaled)[:, 1]
y_pred  = clf.predict(X_test_scaled)

# 4. Accuracy
acc = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {acc:.3f}")

# 5. ROC & AUC
fpr, tpr, _ = roc_curve(y_test, y_proba)
roc_auc    = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, label=f'ROC (AUC = {roc_auc:.3f})')
plt.plot([0, 1], [0, 1], '--', label='Chance')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.show()

# 6. Confusion Matrix
cm = confusion_matrix(y_test, y_pred)

plt.figure()
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()
labels = ['No Card','Card']
plt.xticks([0,1], labels)
plt.yticks([0,1], labels)
plt.xlabel('Predicted')
plt.ylabel('True')

# Annotate counts
thresh = cm.max() / 2
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, cm[i, j],
                 ha='center', va='center',
                 color='white' if cm[i, j] > thresh else 'black')
plt.show()

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# -*- coding: utf-8 -*-
import ipywidgets as widgets
from IPython.display import display, clear_output
import pandas as pd
import io
import numpy as np
from sklearn.preprocessing import LabelEncoder

class CustomerPredictor:
    def __init__(self, model, scaler, top_features, country_rank, income_map,
                 education_map, occupation_options, marital_encoder):
        self.model = model
        self.scaler = scaler
        self.top_features = top_features
        self.country_rank = country_rank
        self.income_map = income_map
        self.education_map = education_map
        self.occupation_options = occupation_options
        self.marital_encoder = marital_encoder
        self.create_ui()

    def create_ui(self):
        # Get valid marital status options from encoder
        marital_options = self.marital_encoder.classes_.tolist()

        # Create input widgets using only trained categories
        self.input_widgets = {
            'CUST_MARITAL_STATUS': widgets.Dropdown(
                description='Marital Status',
                options=marital_options,
                value=marital_options[0]
            ),
            'HOUSEHOLD_SIZE': widgets.IntSlider(
                description='Household Size',
                min=1, max=9, value=2
            ),
            'YRS_RESIDENCE': widgets.IntSlider(
                description='Years Residence',
                min=0, max=50, value=5
            ),
            'AGE': widgets.IntSlider(
                description='Age',
                min=18, max=100, value=30
            ),
            'EDUCATION': widgets.Dropdown(
                description='Education',
                options=list(self.education_map.keys()),
                value=list(self.education_map.keys())[0]
            ),
            'OCCUPATION': widgets.Dropdown(
                description='Occupation',
                options=self.occupation_options,
                value=self.occupation_options[0]
            ),
            'HOME_THEATER_PACKAGE': widgets.Dropdown(
                description='Home Theater',
                options=[('No', 0), ('Yes', 1)],
                value=0
            ),
            'CUST_GENDER': widgets.Dropdown(
                description='Gender',
                options=[('Female', 'F'), ('Male', 'M')],
                value='M'
            ),
            'Y_BOX_GAMES': widgets.Dropdown(
                description='Y-Box Games',
                options=[('No', 0), ('Yes', 1)],
                value=0
            ),
            'COUNTRY_NAME': widgets.Dropdown(
                description='Country',
                options=list(self.country_rank.keys()),
                value=list(self.country_rank.keys())[0]
            )
        }

        # Create buttons and output
        self.predict_button = widgets.Button(description="Predict from Input")
        self.upload_button = widgets.Button(description="Predict from CSV")
        self.file_upload = widgets.FileUpload(accept='.csv')
        self.output = widgets.Output()

        # Arrange UI elements
        input_box = widgets.VBox([
            widgets.Label("Customer Details:"),
            *self.input_widgets.values(),
            self.predict_button
        ])

        upload_box = widgets.VBox([
            widgets.Label("Or upload CSV:"),
            self.file_upload,
            self.upload_button
        ])

        display(widgets.HBox([input_box, upload_box]))
        display(self.output)

        # Set up event handlers
        self.predict_button.on_click(self.predict_from_input)
        self.upload_button.on_click(self.predict_from_file)

    def preprocess_input(self, input_df):
        # Create a working copy of the input data
        processed_df = input_df.copy()

        # Handle marital status encoding
        valid_marital = set(self.marital_encoder.classes_)
        processed_df['CUST_MARITAL_STATUS'] = processed_df['CUST_MARITAL_STATUS'].apply(
            lambda x: x if x in valid_marital else 'Unknown'
        )
        processed_df['CUST_MARITAL_STATUS'] = self.marital_encoder.transform(
            processed_df['CUST_MARITAL_STATUS']
        )

        # Apply other encodings
        processed_df['EDUCATION'] = processed_df['EDUCATION'].map(self.education_map)
        processed_df['COUNTRY_NAME'] = processed_df['COUNTRY_NAME'].map(self.country_rank)
        processed_df['CUST_GENDER'] = processed_df['CUST_GENDER'].map({'F': 0, 'M': 1})

        # One-hot encode occupation
        occupation_dummies = pd.get_dummies(processed_df['OCCUPATION'], prefix='OCCUPATION')
        for occ in self.occupation_options:
            col_name = f'OCCUPATION_{occ}'
            processed_df[col_name] = occupation_dummies.get(col_name, 0)

        # Ensure all required columns are present
        for feature in self.top_features:
            if feature not in processed_df.columns:
                processed_df[feature] = 0

        # Select and order features exactly as model expects
        processed = processed_df[self.top_features]

        # Convert to numeric and handle missing values
        processed = processed.apply(pd.to_numeric, errors='coerce').fillna(0)

        # Scale features
        scaled = self.scaler.transform(processed)
        return scaled

    def predict_from_input(self, b):
        with self.output:
            clear_output()
            try:
                # Create input DataFrame from widgets
                input_data = {k: [w.value] for k, w in self.input_widgets.items()}
                input_df = pd.DataFrame(input_data)

                # Preprocess and predict
                processed = self.preprocess_input(input_df)
                proba = self.model.predict_proba(processed)[0][1]
                prediction = self.model.predict(processed)[0]

                # Display results
                print("=== Prediction Result ===")
                print(f"High-value Probability: {proba:.2%}")
                print(f"Predicted Class: {'High-value Customer' if prediction == 1 else 'Low-value Customer'}")
                print("\nProcessed Input Features:")
                display(pd.DataFrame(processed, columns=self.top_features))

            except Exception as e:
                print(f"Prediction Error: {str(e)}")

    def predict_from_file(self, b):
        with self.output:
            clear_output()
            try:
                if not self.file_upload.value:
                    print("Please upload a CSV file first!")
                    return

                # Read uploaded file
                content = next(iter(self.file_upload.value.values()))['content']
                batch_df = pd.read_csv(io.BytesIO(content))

                # Preprocess and predict
                processed = self.preprocess_input(batch_df)
                batch_df['Predicted_Probability'] = self.model.predict_proba(processed)[:, 1]
                batch_df['Predicted_Class'] = self.model.predict(processed)

                # Save and display results
                batch_df.to_csv('predictions.csv', index=False)
                print(f"Successfully processed {len(batch_df)} records")
                print("\nSample predictions:")
                display(batch_df.head())
                print("\nFull results saved to predictions.csv")

            except Exception as e:
                print(f"File Prediction Error: {str(e)}")

# Example initialization after model training
# Assuming these variables are created during training:
# clf = trained LogisticRegression model
# scaler = fitted StandardScaler
# top_10 = list of top 10 features
# country_rank = country frequency mapping
# income_level_map = income category mapping
# education_map = education level mapping
# occupation_categories = list of occupation categories
# le = fitted LabelEncoder for marital status

predictor = CustomerPredictor(
    model=clf,
    scaler=scaler,
    top_features=top_10,
    country_rank=country_rank,
    income_map=income_level_map,
    education_map=education_map,
    occupation_options=df_copy1['OCCUPATION'].unique(),
    marital_encoder=le
)